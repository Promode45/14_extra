---
title: "XG boost"
format: html
---


  

**Training a model by fine-tuning its hyper-parameters**.

** Boosting hyperparameters 
- Number of trees - The total number of trees in the ensemble 
- Learning rate - Determines the contribution of each tree on the final outcome. Default value is 0.1,, but anywhere between 0.05 - 0.2 should work. 

** Tree parameters 
- Tree depth 
- Minimum number of observations in terminal nodes. 
  

# Setup  
```{r}
#| message: false
#| warning: false

#install.packages("xgboost")
#install.packages("doParallel")
library(tidymodels)
library(tidyverse)
library(vip)
library(ranger)
library(finetune)
library(xgboost)
library(dials)
library(doParallel)
```

```{r weather}
weather <- read_csv("../data/weather_monthsum.csv")

weather
```

# ML workflow  
We're going to use the same workflow as we used for elastic net.   

## 1. Pre-processing  
Here's where we perform **data split** and **data processing**.  

### a. Data split  
For data split, let's use **70% training / 30% testing**.

```{r weather_split}
# Setting seed to get reproducible results  
set.seed(931735)

# Setting split level  
weather_split <- initial_split(weather, 
                               prop = .7)

weather_split
```


```{r weather_train}
# Setting train set 
weather_train <- training(weather_split)

weather_train
```
How many observations?

```{r weather_test}
# Setting test split
weather_test <- testing(weather_split)

weather_test
```
How many observations?  

Now, we put our **test set** aside and continue with our **train set** for training.  

  
### b. Data processing  
Before training, we need to perform some processing steps, like  
  - **normalizing**  
  - **removing unimportant variables**  
  - dropping NAs  
  - performing PCA on the go  
  - removing columns with single value  
  - others?  

For that, we'll create a **recipe** of these processing steps. 

This recipe will then be applied now to the **train data**, and easily applied to the **test data** when we bring it back at the end.

Creating a recipe is as easy way to port your processing steps for other data sets without needing to repeat code, and also only considering the data it is being applied to.  

You can find all available recipe step options here: https://tidymodels.github.io/recipes/reference/index.html

> Differently from other models, xgboost requires a matrix input for features and the response to be a vector.  

```{r weather_recipe}
weather_recipe <-
  # Defining predicted and predictor variables
  recipe(strength_gtex ~ .,
         data = weather_train) %>%
  step_integer(all_nominal()) %>% 
  # Removing year and site  
    step_rm(year, site, matches("Jan|Feb|Mar|Apr|Nov|Dec")) #%>%
#prep(traing = weather_train, retain = T) %>% 
  #juice()
  # Normalizing all numeric variables except predicted variable
  #step_normalize(all_numeric(), -all_outcomes())
#x <- as.matrix(weather_recipe[setdiff(names(weather_recipe), "strength_gtex")])
#y <- weather_recipe$strength_gtex
weather_recipe
```

Now that we have our recipe ready, we **need to apply it** to the training data in a process called prepping:

```{r weather_prep}
weather_prep <- weather_recipe %>%
  prep()

weather_prep

```


Now, we're ready to start the model training process!

## 2. Training  
### a. Model specification  
  
  
```{r xg_spec}
xg_spec <- boost_tree(
  trees = tune(), 
  tree_depth = tune(), min_n = tune(),
  loss_reduction = tune(),
  sample_size = tune(), mtry = tune(),
  learn_rate = tune()
  )%>% 
     #Specify the engine
set_engine("xgboost"
          ) %>% 
    # Specifying mode  
set_mode("regression")

xg_spec
```

Notice how the main arguments above do not have a value **yet**, because they will be tuned.  

### b. Hyper-parameter tuning  


Let's define our resampling strategy below, using a 5-fold cross validation approach:  
```{r resampling_foldcv}
set.seed(34549)
resampling_foldcv <- vfold_cv(weather_train, 
                              v = 5)

resampling_foldcv
resampling_foldcv$splits[[1]]
resampling_foldcv$splits[[2]]
```





In the algorithm below, we are asking for 50 iterations.  
```{r}
xgb_grid <- grid_latin_hypercube(
  trees(),
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), weather_train),
  learn_rate(), 
  size = 50
)
```

```{r}
xgb_wf <- workflow() %>% 
  add_recipe(weather_recipe) %>% 
  add_model(xg_spec)
```


```{r}
doParallel::registerDoParallel()
set.seed(76544)
xgb_res <- tune_grid(
  xgb_wf,
  resamples = resampling_foldcv,
  grid = xgb_grid,
  control = control_grid(save_pred = T)
  )

xgb_res[[3]]

```


```{r rf_grid_result}
collect_metrics(xgb_res)

xgb_res %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  select(mean, mtry:sample_size) %>%
  pivot_longer(mtry:sample_size,
               values_to = "value",
               names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "rmse")


```
```{r}
show_best(xgb_res, metric = "rmse")
```

```{r}
best_rmse <- select_best(xgb_res, metric = "rsq")
```

```{r}
final_xgb <- finalize_workflow(
  xgb_wf,
  best_rmse
)
```

Final model fit:

```{r}
final_fit <- last_fit(final_xgb, weather_split)
collect_metrics(final_fit)
```
Predicted v/s observed : 


```{r}
final_fit %>% 
  collect_predictions() %>%
  ggplot(aes(strength_gtex, .pred))+
  geom_point()+
  geom_abline()+
  geom_smooth()
```

___________________________________




learning rate is a parameter that determines the step size at each iteration while moving towards a global minimum. 


Metrics on **train set** (for curiosity and compare to test set):  
```{r}
# RMSE
final_xgb %>%
  fit(strength_gtex ~ .,
      data = bake(weather_prep, 
                  weather_train)) %>%
  augment(new_data = bake(weather_prep, 
                          weather_train)) %>% 
  rmse(strength_gtex, .pred) %>%
  bind_rows(
    
    
    # R2
    final_spec %>%
      fit(strength_gtex ~ .,
          data = bake(weather_prep, 
                      weather_train)) %>%
      augment(new_data = bake(weather_prep, 
                              weather_train)) %>% 
      rsq(strength_gtex, .pred)
    
  )

```
Variable importance: 


The variables with the **largest average decrease in accuracy** are considered **most important**.  



```{r}
final_xgb %>% 
  fit(data = weather_train) %>% 
  pull_workflow_fit() %>% 
  vip(geom = "point")
```



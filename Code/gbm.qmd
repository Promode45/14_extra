---
title: "XG boost"
format: html
---


  

**Training a model by fine-tuning its hyper-parameters**.

** Boosting hyperparameters 
- Number of trees - The total number of trees in the ensemble 
- Learning rate - Determines the contribution of each tree on the final outcome. Default value is 0.1,, but anywhere between 0.05 - 0.2 should work. 

** Tree parameters 
- Tree depth 
- Minimum number of observations in terminal nodes. 
  

# Setup  
```{r}
#| message: false
#| warning: false

#install.packages("xgboost")

library(tidymodels)
library(tidyverse)
library(vip)
library(ranger)
library(finetune)
library(xgboost)
```

```{r weather}
weather <- read_csv("../data/weather_monthsum.csv")

weather
```

# ML workflow  
We're going to use the same workflow as we used for elastic net.   

## 1. Pre-processing  
Here's where we perform **data split** and **data processing**.  

### a. Data split  
For data split, let's use **70% training / 30% testing**.

```{r weather_split}
# Setting seed to get reproducible results  
set.seed(931735)

# Setting split level  
weather_split <- initial_split(weather, 
                               prop = .7)

weather_split
```


```{r weather_train}
# Setting train set 
weather_train <- training(weather_split)

weather_train
```
How many observations?

```{r weather_test}
# Setting test split
weather_test <- testing(weather_split)

weather_test
```
How many observations?  

Now, we put our **test set** aside and continue with our **train set** for training.  

  
### b. Data processing  
Before training, we need to perform some processing steps, like  
  - **normalizing**  
  - **removing unimportant variables**  
  - dropping NAs  
  - performing PCA on the go  
  - removing columns with single value  
  - others?  

For that, we'll create a **recipe** of these processing steps. 

This recipe will then be applied now to the **train data**, and easily applied to the **test data** when we bring it back at the end.

Creating a recipe is as easy way to port your processing steps for other data sets without needing to repeat code, and also only considering the data it is being applied to.  

You can find all available recipe step options here: https://tidymodels.github.io/recipes/reference/index.html

> Differently from other models, xgboost requires a matrix input for features and the response to be a vector.  

```{r weather_recipe}
weather_recipe <-
  # Defining predicted and predictor variables
  recipe(strength_gtex ~ .,
         data = weather_train) %>%
  # Removing year and site  
    step_rm(year, site, matches("Jan|Feb|Mar|Apr|Nov|Dec")) #%>%
  # Normalizing all numeric variables except predicted variable
  #step_normalize(all_numeric(), -all_outcomes())

weather_recipe
```

Now that we have our recipe ready, we **need to apply it** to the training data in a process called prepping:

```{r weather_prep}
weather_prep <- weather_recipe %>%
  prep()

weather_prep
x <- as.matrix(weather_prep[setdiff(names(weather_prep), "strength_gtex")])
y <- weather_prep$strength_gtex
```


Now, we're ready to start the model training process!

## 2. Training  
### a. Model specification  
  
  
```{r rf_spec}
xg_spec <- 
  # Specifying rf as our model type, asking to tune the hyperparameters
boost_tree(
  trees = tune(),
  tree_depth = tune(),
  min_n = tune(),
  loss_reduction =tune(),
  sample_size = tune(),
  mtry = tune(),
  learn_rate = tune()
) %>% 
    # Specify the engine
set_engine("xgboost") %>% 
    # Specifying mode  
set_mode("regression")
xg_spec
```

Notice how the main arguments above do not have a value **yet**, because they will be tuned.  

### b. Hyper-parameter tuning  
For our iterative search, we need:  
  - Our model specification (`rf_spec`)  
  - The recipe (`weather_recipe`)  
  - Our **resampling strategy** (don't have yet)  

> Notice that for rf we do not need to specify the parameter information, as we needed for CIT. The reason is that for rf, all hyperparameters to be tuned are specified at the model level, whereas for CIT one was at model level and one was at the engine level. Engine level hyperparameters need to be "finalized" and have their range of values set up before they can be used in search methods.  

Let's define our resampling strategy below, using a 5-fold cross validation approach:  
```{r resampling_foldcv}
set.seed(34549)
resampling_foldcv <- vfold_cv(weather_train, 
                              v = 5)

resampling_foldcv
resampling_foldcv$splits[[1]]
resampling_foldcv$splits[[2]]
```
On each fold, we'll use **390** observations for training and **98** observations to assess performance.    

Now, let's perform the search below.  

We will use an iterative search algorithm called **simulated annealing**.  

Here's how it works:  
![](https://www.tmwr.org/figures/iterative-neighborhood-1.png)
  - In the example above, mixture and penalty from an elastic net model are being tuned.  

  - It finds a candidate value of hyperparameters and their associated rmse to start (iteration 1).  

  - It establishes a radius around the first proposal, and randomly chooses a new set of values within that radius.  
  
  - If this achieves better results than the previous parameters, it is accepted as the new best and the process continues. If the results are worse than the previous value the search procedure may still use this parameter to define further steps. 
  
  - After a given number of iterations, the algorithm stops and provides a list of the best models and their hyperparameters.  

In the algorithm below, we are asking for 50 iterations.  
```{r}
xg_param <- xg_spec %>% 
  extract_parameter_set_dials() %>% 
  update(gamma = gamma())

```

```{r rf_grid_result}
set.seed(76544)
xg_grid_result <- tune_sim_anneal(object = xg_spec,
                     preprocessor = weather_recipe,
                     resamples = resampling_foldcv,
                    #param_info = rf_param,
                    iter = 20
                     )


xg_grid_result
xg_grid_result$.metrics[[2]]
collect_metrics(xg_grid_result)
```
Notice how we have a column for iterations.  
The first iteration uses a sensible value for the hyper-parameters, and then starts "walking" the parameter space in the direction of greatest improvement.  

Let's collect a summary of metrics (across all folds, for each iteration), and plot them.  

Firs, RMSE (lower is better):
```{r RMSE}
xg_grid_result %>% 
  collect_metrics() %>% 
  filter(.metric == "rmse") %>% 
  pivot_longer(mtry:sample_size,
               names_to = "parameter",
               values_to = "value") %>% 
  ggplot(aes(value,mean))+
  geom_path(group = 1)+
  geom_point(aes(color = mean),
             size = 3)+ 
  facet_wrap(~parameter, scales = "free_x")+
  geom_text(aes(label = .iter), nudge_x = .0005)+
  labs(title = "RMSE")
```

What tree_depth and min criterion values created lowest RMSE?  

Now, let's look into R2 (higher is better):  

```{r R2}
xg_grid_result %>%
  collect_metrics() %>%
  filter(.metric == "rsq") %>%
  select(mean, mtry:sample_size) %>% 
  pivot_longer(mtry:sample_size,
               values_to = "value",
               names_to = "parameter") %>% 
  ggplot(aes(x = value, 
             y = mean,
             color = parameter
             )) +
  geom_point(size = 3, show.legend = F) + 
  facet_wrap(~parameter, scales = "free_x")+
  labs(title = "RSQ")

```

> Previously, we selected the single best model. Now, let's select the best model within one std error of the metric, so we choose a model among the top ones that is more parsimonious.  

```{r}
# Based on lowest RMSE
best_rmse <- xg_grid_result %>% 
  select_best(metric = "rmse")
best_rmse

```

```{r}
# Based on greatest R2
best_r2 <- xg_grid_result %>% 
  select_best(
               metric = "rsq",
              )


best_r2
show_best(xg_grid_result)

```
learning rate is a parameter that determines the step size at each iteration while moving towards a global minimum. 

```{r final_spec}
final_spec <- boost_tree(
  trees = best_rmse$trees,
  tree_depth = 9,
  min_n = best_r2$min_n,
  loss_reduction = best_r2$loss_reduction,
  sample_size = best_r2$sample_size,
  mtry = best_r2$mtry,
  learn_rate = best_r2$learn_rate
) %>% 
  # Specify the engine
  set_engine("xgboost") %>% 

    # Specifying mode  
  set_mode("regression")
  

final_spec
```

## 3. Validation  
Now that we determined our best model, let's do our **last fit**.

This means 2 things:  
  - Traninig the optimum hyperparameter values on the **entire training set**  
  - Using it to **predict** on the **test set**  

These 2 steps can be completed in one function, as below:  

```{r final_fit}
final_fit <- last_fit(final_spec,
                weather_recipe,
                split = weather_split)

final_fit %>%
  collect_predictions()
```

Metrics on the **test set**:
```{r}
final_fit %>%
  collect_metrics()
```

Metrics on **train set** (for curiosity and compare to test set):  
```{r}
# RMSE
final_spec %>%
  fit(strength_gtex ~ .,
      data = bake(weather_prep, 
                  weather_train)) %>%
  augment(new_data = bake(weather_prep, 
                          weather_train)) %>% 
  rmse(strength_gtex, .pred) %>%
  bind_rows(
    
    
    # R2
    final_spec %>%
      fit(strength_gtex ~ .,
          data = bake(weather_prep, 
                      weather_train)) %>%
      augment(new_data = bake(weather_prep, 
                              weather_train)) %>% 
      rsq(strength_gtex, .pred)
    
  )

```
How does metrics on test compare to metrics on train?  

Predicted vs. observed plot:  
```{r}
final_fit %>%
  collect_predictions() %>%
  ggplot(aes(x = strength_gtex,
             y = .pred)) +
  geom_point() +
  geom_abline() +
  geom_smooth(method = "lm") +
  scale_x_continuous(limits = c(20, 40)) +
  scale_y_continuous(limits = c(20, 40)) 
```
```{r}
vip::vip(final_fit)
```

Variable importance: 

The importance metric we are evaluating here is **permutation**. 

In the permutation-based approach, for each tree, the out- of-bag sample is passed down the tree and the prediction accuracy is recorded.   

Then the values for each variable (one at a time) are randomly permuted and the accuracy is again computed.   

The decrease in accuracy as a result of this randomly shuffling of feature values is averaged over all the trees for each predictor.   

The variables with the **largest average decrease in accuracy** are considered **most important**.  

```{r}
final_spec %>%
  fit(strength_gtex ~ .,
         data = bake(weather_prep, weather)) %>%
    vi() %>%
  mutate(
    Variable = fct_reorder(Variable, 
                           Importance)
  ) %>%
  ggplot(aes(x = Importance, 
             y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)
    
```

**Therefore, solar radiation in July and August, and minimum temperature in June were the most important variables affecting cotton fiber strength.**  

# Summary  
In this exercise, we covered: 
  - Random forest algorithm    
  - Set up a ML workflow to train an rf model  
  - Used `recipes` to process data
  - Used `rsamples` to split data  
  - Used **iterative search** to find the best values for mas_depth and min_criterion    
  - Used 5-fold cross validation as the resampling method  
  - Used both R2 and RMSE as the metrics to select best model  
  - Once final model was determined, used it to predict **test set**  
  - Evaluated it with predicted vs. observed plot, R2 and RMSE metrics, variable importance, and tree plot    

# Further resources:  

  - Tidy modeling with R: https://www.tmwr.org  
  - Tidy modeling with R book club: https://r4ds.github.io/bookclub-tmwr/  
  
    - Hands-on ML in R: https://bradleyboehmke.github.io/HOML/  
  
  - ML for social scientists: https://cimentadaj.github.io/ml_socsci/  

# Quiz  
Go on eLC.  

# TEVAL  

Please take 5 min to respond to the TEVAL.  
Your feedback is really important for me to know what worked, what didn't, and improve for next time.  

Thanks!

Link: https://webapps.franklin.uga.edu/evaluation/?_ga=2.232813078.929468690.1713977543-462720075.1710361396&_gl=1*o88gkf*_ga*NDYyNzIwMDc1LjE3MTAzNjEzOTY.*_ga_3ZLXDSEKEC*MTcxNDAxMzg4Mi4xLjEuMTcxNDAxNDM4MC4wLjAuMA..



